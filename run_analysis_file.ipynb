{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running analysis on BugsInPy Dataset\n",
    "## IMPORTANT: You need at least a T4 GPU\n",
    "### Overall flow of the notebook:\n",
    "\n",
    "1. Clone the project\n",
    "2. Install dependencies\n",
    "3. Chunk the error traces and make an index\n",
    "4. Search the index for errotraces against code chunks\n",
    "5. Generate statistics based on the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all of the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooria/bootcamp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scripts.embedding import model, MODEL_NAME, BATCH_SIZE, embed, index_embeddings\n",
    "from scripts.bugsinpy_utils import *\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the directories exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.abspath(f\"tmp/ast/results\"), exist_ok=True)\n",
    "K = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRR & MPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_precision(y_true):\n",
    "    hits = 0\n",
    "    sum_precisions = 0.0\n",
    "    for i, rel in enumerate(y_true):\n",
    "        if rel:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / (i + 1)\n",
    "    return sum_precisions / max(hits, 1)\n",
    "\n",
    "def compute_reciprocal_rank(y_true):\n",
    "    for i, rel in enumerate(y_true, start=1):  # ranks start at 1\n",
    "        if rel:\n",
    "            return 1.0 / i\n",
    "    return 0.0  # no relevant item found in top K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed error traces and match them against code chunk embeddings\n",
    "\n",
    "\n",
    "\n",
    "Running each eligible bug through the model and embedding them, after then running cosine similarity to determine which files they think it should be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 23/23 [00:14<00:00,  1.58it/s]\n",
      "Embedding: 100%|██████████| 124/124 [01:43<00:00,  1.19it/s]\n",
      "Embedding: 100%|██████████| 34/34 [00:06<00:00,  5.47it/s]\n",
      "Embedding: 100%|██████████| 30/30 [00:15<00:00,  1.90it/s]\n",
      "Embedding: 100%|██████████| 19/19 [00:06<00:00,  2.89it/s]\n",
      "Embedding: 100%|██████████| 37/37 [00:09<00:00,  3.80it/s]\n",
      "Embedding: 100%|██████████| 30/30 [00:20<00:00,  1.50it/s]\n",
      "Embedding: 100%|██████████| 34/34 [00:26<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "projects = get_projects()\n",
    "all_ap = []\n",
    "all_rr = []\n",
    "\n",
    "for project in projects:\n",
    "    bugs = get_bugs(project)\n",
    "    # embedding_index_path = f\"tmp/ast/embeddings/index_{project}.faiss\"\n",
    "    bug_result_path = os.path.abspath(f\"tmp/ast/results/bug_results_{project}.json\")\n",
    "    filtered_bugs = []\n",
    "    error_texts = []\n",
    "\n",
    "    # First pass: filter bugs and collect error traces\n",
    "    for bug in bugs:\n",
    "            \n",
    "        changed_files = parse_changed_files(project, bug)\n",
    "        if len(changed_files) > 1:\n",
    "            continue\n",
    "        info = get_bug_info(project, bug)\n",
    "        error = extract_python_tracebacks(project, bug)\n",
    "        if error:\n",
    "            filtered_bugs.append(bug)\n",
    "            error_texts.append(error)\n",
    "\n",
    "    # Batch encode\n",
    "    if error_texts:\n",
    "        error_embeddings = embed(error_texts, batch_size=BATCH_SIZE, show_progress_bar=True)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for bug, emb in zip(filtered_bugs, error_embeddings):\n",
    "            code_chunks_path = f\"dataset/{project}/{bug}/code_chunks.json\"\n",
    "            with open(code_chunks_path, \"r\") as f:\n",
    "                code_chunks = json.loads(f.read())\n",
    "            embedding_path = f\"dataset/{project}/{bug}/embedding.npy\"\n",
    "            embeddings = np.load(embedding_path)\n",
    "            index = index_embeddings(embeddings)\n",
    "            D, I = index.search(np.array([emb]).astype(\"float32\"), k=K)\n",
    "            search_results = {\"index\": bug, \"files\": []}\n",
    "            for idx in I[0]:\n",
    "                result = code_chunks[idx]\n",
    "                search_results[\"files\"].append(\n",
    "                    {\"file\": result[\"file\"], \"function\": result[\"name\"]}\n",
    "                )\n",
    "            output.append(search_results)\n",
    "            \n",
    "            changed_file = parse_changed_files(project, bug)[0].split(\"/\")[-1]\n",
    "            y_true = [1 if code_chunks[idx][\"file\"].split(\"/\")[-1] == changed_file else 0 for idx in I[0]]\n",
    "            ap = compute_average_precision(y_true)\n",
    "            rr = compute_reciprocal_rank(y_true)\n",
    "\n",
    "            all_ap.append(ap)\n",
    "            all_rr.append(rr)\n",
    "\n",
    "        with open(bug_result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate results in a human readable way \"results_K.json\"\n",
    "\n",
    "Using that predictions to calculate success rate and show where each search was successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_folder = os.path.abspath(\"tmp/ast/results\")\n",
    "results_files = os.listdir(results_folder)\n",
    "def analyze(k):\n",
    "    results = {\n",
    "        \"K\": k,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"searches_failed\": [],  # [{project_name, bug_id, detected_files: list, actual_file}]\n",
    "        \"searches_passed\": [],  # [{project_name, bug_id, detected_files: list, actual_files}]\n",
    "        \"success_rate\": 0,  # out of 100 (including bugs skipped)\n",
    "        \"MAP\": np.mean(all_ap),\n",
    "        \"MRR\": np.mean(all_rr),\n",
    "        \"success_projects\": {},  # {project_name, success_rate, success_rate_no_skip}\n",
    "    }\n",
    "    success_projects_tmp = {}\n",
    "    count = 0\n",
    "    maximum_bugs = 0\n",
    "    for project in results_files:\n",
    "        project_name = project.replace(\"bug_results_\", \"\").replace(\".json\", \"\")\n",
    "        bugs = get_bugs(project_name)\n",
    "        maximum_bugs += len(bugs)\n",
    "        success_projects_tmp[project_name] = {\n",
    "            \"failed\": 0,\n",
    "            \"passed\": 0,\n",
    "            \"total\": 0,\n",
    "        }\n",
    "\n",
    "        with open(f\"{results_folder}/{project}\", \"r\", encoding=\"utf-8\") as result_file:\n",
    "            data = json.loads(result_file.read())\n",
    "            for search_result in data:\n",
    "                changed_file = parse_changed_files(project_name, search_result[\"index\"])[0].split(\"/\")[-1]\n",
    "                files_predicted = [\n",
    "                    obj[\"file\"].split(\"/\")[-1] for obj in search_result[\"files\"][:k]\n",
    "                ]\n",
    "                if changed_file in files_predicted:\n",
    "                    results[\"searches_passed\"].append(\n",
    "                        {\n",
    "                            \"project_name\": project_name,\n",
    "                            \"bug_id\": search_result[\"index\"],\n",
    "                            \"detected_files\": files_predicted,\n",
    "                            \"actual_file\": changed_file,\n",
    "                        }\n",
    "                    )\n",
    "                    success_projects_tmp[project_name][\"passed\"] += 1\n",
    "                    success_projects_tmp[project_name][\"total\"] += 1\n",
    "                else:\n",
    "                    results[\"searches_failed\"].append(\n",
    "                        {\n",
    "                            \"project_name\": project_name,\n",
    "                            \"bug_id\": search_result[\"index\"],\n",
    "                            \"detected_files\": files_predicted,\n",
    "                            \"actual_file\": changed_file,\n",
    "                        }\n",
    "                    )\n",
    "                    success_projects_tmp[project_name][\"failed\"] += 1\n",
    "                    success_projects_tmp[project_name][\"total\"] += 1\n",
    "                    \n",
    "\n",
    "\n",
    "        searches_counted = (\n",
    "            success_projects_tmp[project_name][\"passed\"]\n",
    "            + success_projects_tmp[project_name][\"failed\"]\n",
    "        )\n",
    "\n",
    "        results[\"success_projects\"][project_name] = {\n",
    "            \"success_rate\": success_projects_tmp[project_name][\"passed\"] / searches_counted,\n",
    "        } | success_projects_tmp[project_name]\n",
    "\n",
    "    results[\"success_rate\"] = len(results[\"searches_passed\"]) / (len(results[\"searches_failed\"]) + len(results[\"searches_passed\"]))\n",
    "    with open(os.path.abspath(f\"results_{k}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(json.dumps(results, indent=2))\n",
    "        \n",
    "analyze(1) # 1 = K value\n",
    "analyze(5) # 5 = K value\n",
    "analyze(10) # 10 = K value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An easier way to show the results and is used to create tables in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project         Passed Failed  Total  Success Rate\n",
      "==================================================\n",
      "0.9411764705882353\n",
      "0.6470588235294118\n",
      "0.6521739130434783\n",
      "0.7894736842105263\n",
      "0.9\n",
      "0.9459459459459459\n",
      "0.6129032258064516\n",
      "1.0\n",
      "\n",
      "Overall Success Rate: 76.13%\n"
     ]
    }
   ],
   "source": [
    "Ks = [1, 5, 10]\n",
    "\n",
    "def show(k: int):\n",
    "    # Load your data\n",
    "    with open(f\"results_{k}.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Desired project order\n",
    "    ordered_projects = [\n",
    "        \"youtube-dl\",\n",
    "        \"keras\",\n",
    "        \"matplotlib\",\n",
    "        \"black\",\n",
    "        \"thefuck\",\n",
    "        \"scrapy\",\n",
    "        \"pandas\",\n",
    "        \"luigi\",\n",
    "        \"ansible\"\n",
    "    ]\n",
    "\n",
    "    # Print header\n",
    "    print(f\"{'Project':<15} {'Passed':>6} {'Failed':>6} {'Total':>6} {'Success Rate':>13}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Print each project in order\n",
    "    for project in ordered_projects:\n",
    "        stats = data[\"success_projects\"][project]\n",
    "        passed = stats[\"passed\"]\n",
    "        failed = stats[\"failed\"]\n",
    "        total = stats[\"total\"]\n",
    "        rate = stats[\"success_rate\"]\n",
    "        print(f\"{project:<15} {passed:>6} {failed:>6} {total:>6} {rate:>12.2f}%\")\n",
    "\n",
    "    # Print overall success rate\n",
    "    print(\"\\nOverall Success Rate:\", f\"{data['success_rate'] * 100:.2f}%\")\n",
    "\n",
    "for k in Ks:\n",
    "    show(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

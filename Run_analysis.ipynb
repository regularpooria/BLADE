{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all of the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooria/bootcamp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from scripts.embedding import model, MODEL_NAME, BATCH_SIZE, embed, index_embeddings\n",
    "from scripts.bugsinpy_utils import *\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the directories exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.abspath(f\"tmp/ast/results\"), exist_ok=True)\n",
    "K = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running each eligible bug through the model and embedding them, after then running cosine similarity to determine which files they think it should be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 4/4 [00:57<00:00, 14.39s/it]\n"
     ]
    }
   ],
   "source": [
    "projects = get_projects()\n",
    "for project in projects:\n",
    "    bugs = get_bugs(project)\n",
    "    # embedding_index_path = f\"tmp/ast/embeddings/index_{project}.faiss\"\n",
    "    bug_result_path = os.path.abspath(f\"tmp/ast/results/bug_results_{project}.json\")\n",
    "    filtered_bugs = []\n",
    "    error_texts = []\n",
    "\n",
    "    # First pass: filter bugs and collect error traces\n",
    "    for bug in bugs:\n",
    "            \n",
    "        changed_files = parse_changed_files(project, bug)\n",
    "        if len(changed_files) > 1:\n",
    "            continue\n",
    "        info = get_bug_info(project, bug)\n",
    "        error = extract_python_tracebacks(project, bug)\n",
    "        if error:\n",
    "            filtered_bugs.append(bug)\n",
    "            error_texts.append(error)\n",
    "\n",
    "    # Batch encode\n",
    "    if error_texts:\n",
    "        error_embeddings = embed(error_texts, batch_size=BATCH_SIZE, show_progress_bar=True)\n",
    "\n",
    "        output = []\n",
    "        for bug, emb in zip(filtered_bugs, error_embeddings):\n",
    "            code_chunks_path = f\"dataset/{project}/{bug}/code_chunks.json\"\n",
    "            with open(code_chunks_path, \"r\") as f:\n",
    "                code_chunks = json.loads(f.read())\n",
    "            embedding_path = f\"dataset/{project}/{bug}/embedding.npy\"\n",
    "            embeddings = np.load(embedding_path)\n",
    "            index = index_embeddings(embeddings)\n",
    "            D, I = index.search(np.array([emb]).astype(\"float32\"), k=K)\n",
    "            search_results = {\"index\": bug, \"files\": []}\n",
    "            for idx in I[0]:\n",
    "                result = code_chunks[idx]\n",
    "                search_results[\"files\"].append(\n",
    "                    {\"file\": result[\"file\"], \"function\": result[\"name\"]}\n",
    "                )\n",
    "            output.append(search_results)\n",
    "\n",
    "        with open(bug_result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that predictions to calculate success rate and show where each search was successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_folder = os.path.abspath(\"tmp/ast/results\")\n",
    "results_files = os.listdir(results_folder)\n",
    "def analyze(k):\n",
    "    results = {\n",
    "        \"K\": k,\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"searches_failed\": [],  # [{project_name, bug_id, detected_files: list, actual_file}]\n",
    "        \"searches_passed\": [],  # [{project_name, bug_id, detected_files: list, actual_files}]\n",
    "        \"success_rate\": 0,  # out of 100 (including bugs skipped)\n",
    "        \"success_projects\": {},  # {project_name, success_rate, success_rate_no_skip}\n",
    "    }\n",
    "    success_projects_tmp = {}\n",
    "    count = 0\n",
    "    maximum_bugs = 0\n",
    "    for project in results_files:\n",
    "        project_name = project.replace(\"bug_results_\", \"\").replace(\".json\", \"\")\n",
    "        bugs = get_bugs(project_name)\n",
    "        maximum_bugs += len(bugs)\n",
    "        success_projects_tmp[project_name] = {\n",
    "            \"failed\": 0,\n",
    "            \"passed\": 0,\n",
    "            \"total\": 0,\n",
    "        }\n",
    "\n",
    "        with open(f\"{results_folder}/{project}\", \"r\", encoding=\"utf-8\") as result_file:\n",
    "            data = json.loads(result_file.read())\n",
    "            for search_result in data:\n",
    "                changed_file = parse_changed_files(project_name, search_result[\"index\"])[0].split(\"/\")[-1]\n",
    "                files_predicted = [\n",
    "                    obj[\"file\"].split(\"/\")[-1] for obj in search_result[\"files\"][:k]\n",
    "                ]\n",
    "                if changed_file in files_predicted:\n",
    "                    results[\"searches_passed\"].append(\n",
    "                        {\n",
    "                            \"project_name\": project_name,\n",
    "                            \"bug_id\": search_result[\"index\"],\n",
    "                            \"detected_files\": files_predicted,\n",
    "                            \"actual_file\": changed_file,\n",
    "                        }\n",
    "                    )\n",
    "                    success_projects_tmp[project_name][\"passed\"] += 1\n",
    "                    success_projects_tmp[project_name][\"total\"] += 1\n",
    "                else:\n",
    "                    results[\"searches_failed\"].append(\n",
    "                        {\n",
    "                            \"project_name\": project_name,\n",
    "                            \"bug_id\": search_result[\"index\"],\n",
    "                            \"detected_files\": files_predicted,\n",
    "                            \"actual_file\": changed_file,\n",
    "                        }\n",
    "                    )\n",
    "                    success_projects_tmp[project_name][\"failed\"] += 1\n",
    "                    success_projects_tmp[project_name][\"total\"] += 1\n",
    "                    \n",
    "\n",
    "\n",
    "        searches_counted = (\n",
    "            success_projects_tmp[project_name][\"passed\"]\n",
    "            + success_projects_tmp[project_name][\"failed\"]\n",
    "        )\n",
    "\n",
    "        results[\"success_projects\"][project_name] = {\n",
    "            \"success_rate\": success_projects_tmp[project_name][\"passed\"] / searches_counted,\n",
    "        } | success_projects_tmp[project_name]\n",
    "\n",
    "    results[\"success_rate\"] = len(results[\"searches_passed\"]) / (len(results[\"searches_failed\"]) + len(results[\"searches_passed\"]))\n",
    "    with open(os.path.abspath(f\"results_{k}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(json.dumps(results, indent=2))\n",
    "        \n",
    "analyze(5)\n",
    "analyze(10)\n",
    "analyze(15)\n",
    "analyze(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project         Passed Failed  Total  Success Rate\n",
      "==================================================\n",
      "0.9705882352941176\n",
      "0.8235294117647058\n",
      "0.6521739130434783\n",
      "0.9473684210526315\n",
      "0.9\n",
      "1.0\n",
      "0.7258064516129032\n",
      "1.0\n",
      "\n",
      "Overall Success Rate: 83.99%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load your data\n",
    "with open(\"results_20.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Desired project order\n",
    "ordered_projects = [\n",
    "    \"youtube-dl\",\n",
    "    \"keras\",\n",
    "    \"matplotlib\",\n",
    "    \"black\",\n",
    "    \"thefuck\",\n",
    "    \"scrapy\",\n",
    "    \"pandas\",\n",
    "    \"luigi\",\n",
    "]\n",
    "\n",
    "# Print header\n",
    "print(f\"{'Project':<15} {'Passed':>6} {'Failed':>6} {'Total':>6} {'Success Rate':>13}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print each project in order\n",
    "for project in ordered_projects:\n",
    "    stats = data[\"success_projects\"][project]\n",
    "    passed = stats[\"passed\"]\n",
    "    failed = stats[\"failed\"]\n",
    "    total = stats[\"total\"]\n",
    "    rate = stats[\"success_rate\"]\n",
    "    print(rate)\n",
    "\n",
    "# Print overall success rate\n",
    "print(\"\\nOverall Success Rate:\", f\"{data['success_rate'] * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

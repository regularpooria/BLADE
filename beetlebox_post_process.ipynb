{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished cryptomator_cryptomator\n",
      "finished fastify_fastify\n",
      "finished onnx_onnx\n",
      "finished facebook_react\n",
      "finished liquibase_liquibase\n",
      "finished ccxt_ccxt\n",
      "finished SeleniumHQ_selenium\n",
      "finished mozilla_pdf.js\n",
      "finished electron_electron\n",
      "finished ansible_ansible\n",
      "finished protocolbuffers_protobuf\n",
      "finished bitcoin_bitcoin\n",
      "finished google_guava\n",
      "finished apache_dubbo\n",
      "finished nats-io_nats-server\n",
      "finished OpenRefine_OpenRefine\n",
      "finished langchain-ai_langchain\n",
      "finished sveltejs_svelte\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "projects = os.listdir(\"beetlebox_dataset\")\n",
    "\n",
    "for project in projects:\n",
    "    project_path = os.path.join(\"beetlebox_dataset\", project)\n",
    "    if not os.path.isdir(project_path):\n",
    "        continue\n",
    "\n",
    "    chunk_counter = defaultdict(list)\n",
    "    bugs = os.listdir(project_path)\n",
    "\n",
    "    # First pass: count all chunks (by hash) in this project\n",
    "    for bug in bugs:\n",
    "        bug_path = os.path.join(project_path, bug)\n",
    "        if not os.path.isdir(bug_path):\n",
    "            continue\n",
    "        chunk_file = os.path.join(bug_path, \"chunks.json\")\n",
    "        if not os.path.exists(chunk_file):\n",
    "            continue\n",
    "        with open(chunk_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                chunks = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {chunk_file}: {e}\")\n",
    "                continue\n",
    "            for chunk in chunks:\n",
    "                code = chunk.get(\"chunk\")\n",
    "                if not code:\n",
    "                    continue\n",
    "                h = hashlib.sha256(code.encode(\"utf-8\")).hexdigest()\n",
    "                # Only keep a *single* representative chunk per hash (minimize memory)\n",
    "                if len(chunk_counter[h]) == 0:\n",
    "                    chunk_counter[h].append(chunk)\n",
    "                else:\n",
    "                    # We only need to know that it's duplicated, so just append a dummy for counting\n",
    "                    chunk_counter[h].append(None)\n",
    "\n",
    "    # Find all duplicated hashes in this project\n",
    "    duplicated_hashes = {h for h, chunk_list in chunk_counter.items() if len(chunk_list) > 1}\n",
    "\n",
    "    # Save representative duplicated chunks\n",
    "    duplicated_chunks = [\n",
    "        chunk_list[0] for h, chunk_list in chunk_counter.items() if len(chunk_list) > 1\n",
    "    ]\n",
    "\n",
    "    with open(os.path.join(project_path, \"duplicated_chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(duplicated_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Second pass: write unique_chunks.json per bug, subtracting duplicates\n",
    "    for bug in bugs:\n",
    "        bug_path = os.path.join(project_path, bug)\n",
    "        if not os.path.isdir(bug_path):\n",
    "            continue\n",
    "        chunk_file = os.path.join(bug_path, \"chunks.json\")\n",
    "        if not os.path.exists(chunk_file):\n",
    "            continue\n",
    "\n",
    "        unique_chunks = []\n",
    "        with open(chunk_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                chunks = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {chunk_file}: {e}\")\n",
    "                continue\n",
    "            for chunk in chunks:\n",
    "                code = chunk.get(\"chunk\")\n",
    "                if not code:\n",
    "                    continue\n",
    "                h = hashlib.sha256(code.encode(\"utf-8\")).hexdigest()\n",
    "                if h not in duplicated_hashes:\n",
    "                    unique_chunks.append(chunk)\n",
    "\n",
    "        with open(os.path.join(bug_path, \"unique_chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(unique_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"finished {project}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating archive of unique_chunks.json and duplicated_chunks.json under beetlebox_dataset...\n",
      "Archive created: beetlebox.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "DATASET_DIR=\"beetlebox_dataset\"\n",
    "OUTPUT_ARCHIVE=\"beetlebox.tar.gz\"\n",
    "\n",
    "echo \"Creating archive of unique_chunks.json and duplicated_chunks.json under $DATASET_DIR...\"\n",
    "\n",
    "find \"$DATASET_DIR\" -type f \\( -name 'unique_chunks.json' -o -name 'duplicated_chunks.json' \\) > chunk_file_list.txt\n",
    "\n",
    "# Check if any files were found\n",
    "if [ ! -s chunk_file_list.txt ]; then\n",
    "    echo \"No unique_chunks.json or duplicated_chunks.json files found in $DATASET_DIR\"\n",
    "    rm chunk_file_list.txt\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "tar --transform \"s|^./||\" -czf \"$OUTPUT_ARCHIVE\" -T chunk_file_list.txt\n",
    "\n",
    "echo \"Archive created: $OUTPUT_ARCHIVE\"\n",
    "rm chunk_file_list.txt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
